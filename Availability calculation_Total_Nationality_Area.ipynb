{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8e5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34917659",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DATA_FILE = 'C:/Users/511232/Desktop/DSS/MERGING GOOGLESHEETS QUESTIONNAIRES/codes/arabic_questionnaires.xlsx'\n",
    "CRITERIA_FILE = 'C:/Users/511232/Desktop/criterias.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedffb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criteria_dict(criteria_df, key_language='arabic'):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping indicator names to their availability criteria.\n",
    "\n",
    "    Args:\n",
    "        criteria_df (pd.DataFrame): DataFrame containing indicator names and criteria.\n",
    "        key_language (str): 'english' or 'arabic'. Determines which indicator name to use as the key.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping indicator names to their integer criteria.\n",
    "    \"\"\"\n",
    "    if key_language.lower() == 'english':\n",
    "        key_col = 'Indicator_En'\n",
    "    elif key_language.lower() == 'arabic':\n",
    "        key_col = 'Indicator_Ar'\n",
    "    else:\n",
    "        raise ValueError(\"key_language must be 'english' or 'arabic'\")\n",
    "\n",
    "    # Drop rows where the key column is NaN to avoid issues\n",
    "    criteria_df.dropna(subset=[key_col], inplace=True)\n",
    "    \n",
    "    return pd.Series(criteria_df.criteria.values, index=criteria_df[key_col]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d90d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_availability(df, group_cols, criteria_dict, year_col='السنة', indicator_col='المؤشر', window_size=5):\n",
    "    \"\"\"\n",
    "    1. indicator_name = group.name[...]\n",
    "    This line just gets the name of the indicator we are working on.\n",
    "    indicator_name = \"Literacy rate\"\n",
    "\n",
    "    2. criteria = criteria_dict.get(indicator_name, 1)\n",
    "    This looks up the \"Literacy rate\" in our criteria dictionary and finds its requirement.\n",
    "    criteria = 2\n",
    "\n",
    "    3. binned_years = pd.cut(...)\n",
    "    This is the categorization step. It takes our list of years and puts each one into a 5-year \"bucket\".\n",
    "\n",
    "    2011 -> [2010, 2015)\n",
    "    2012 -> [2010, 2015)\n",
    "    2016 -> [2015, 2020)\n",
    "    2018 -> [2015, 2020)\n",
    "    2021 -> [2020, 2025)\n",
    "    2022 -> [2020, 2025)\n",
    "    2023 -> [2020, 2025)\n",
    "\n",
    "    4. window_counts = binned_years.value_counts()\n",
    "    This step counts how many data points landed in each bucket.\n",
    "\n",
    "    [2010, 2015): 2\n",
    "    [2015, 2020): 2\n",
    "    [2020, 2025): 3\n",
    "\n",
    "    5. windows_with_sufficient_data = window_counts[window_counts >= criteria]\n",
    "    This is a filter. It looks at our counts and keeps only the buckets where the count is greater than or equal to our criteria (which is 2).\n",
    "\n",
    "    [2010, 2015): Kept (because 2 >= 2)\n",
    "    [2015, 2020): Kept (because 2 >= 2)\n",
    "    [2020, 2025): Kept (because 3 >= 2)\n",
    "\n",
    "    6. sufficient_windows_set = set(windows_with_sufficient_data.index)\n",
    "    This creates a clean, unique list of the windows that passed the filter.\n",
    "    sufficient_windows_set = { [2010, 2015), [2015, 2020), [2020, 2025) }\n",
    "\n",
    "    7. return 1 if len(...) == len(...) else 0\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    # Determine the overall year range and create standard bins\n",
    "    min_year = 2010\n",
    "    max_year = df[year_col].max()\n",
    "    bins = range(min_year, max_year + window_size + 1, window_size)\n",
    "    \n",
    "    # Create a set of all possible windows (bins) that could exist\n",
    "    all_possible_windows = set(pd.cut(pd.Series(range(min_year, max_year + 1)), bins=bins, right=False).dropna().unique())\n",
    "\n",
    "    def check_group(group):\n",
    "        '''group_cols.index(indicator_col):\n",
    "        This finds the position (the index) of our indicator column within the list of grouping columns.\n",
    "\n",
    "        group_cols is ['المؤشر', 'الدولة'].\n",
    "        The position of 'المؤشر' in this list is 0.\n",
    "\n",
    "        group.name:\n",
    "        When pandas groups data, the .name attribute of each group is a tuple containing the values for that specific group.\n",
    "        Since our group is for \"حجم السكان حسب المواطنة\" and \"تونس\", the group.name will be: ('حجم السكان حسب المواطنة', 'تونس').\n",
    "\n",
    "        Putting it together: group.name[0]:\n",
    "\n",
    "        The code now effectively becomes ('حجم السكان حسب المواطنة', 'تونس')[0].\n",
    "        This retrieves the item at position 0 from the tuple.\n",
    "        Result: The variable indicator_name is now set to the string 'حجم السكان حسب المواطنة'.'''\n",
    "        \n",
    "        # Find the name of the current indicator being processed.\n",
    "        indicator_name = group.name[group_cols.index(indicator_col)]\n",
    "        # Get the criteria for the specific indicator, default to 1 if not found\n",
    "        criteria = criteria_dict.get(indicator_name, 1)\n",
    "\n",
    "        binned_years = pd.cut(group[year_col], bins=bins, right=False)\n",
    "        window_counts = binned_years.value_counts()\n",
    "        \n",
    "        # Find windows that meet or exceed the criteria\n",
    "        windows_with_sufficient_data = window_counts[window_counts >= criteria]\n",
    "        sufficient_windows_set = set(windows_with_sufficient_data.index)\n",
    "        \n",
    "        # If all possible windows are present in the set of sufficient windows, it's available\n",
    "        return 1 if len(sufficient_windows_set) == len(all_possible_windows) else 0\n",
    "\n",
    "    return df.groupby(group_cols).apply(check_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c46f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read successfully.\n",
      "Criteria dictionary created with 8 entries.\n",
      "Calculating general availability...\n",
      "Calculating nationality availability...\n",
      "Calculating area availability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\511232\\AppData\\Local\\Temp\\ipykernel_1820\\795136313.py:85: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(group_cols).apply(check_group)\n",
      "C:\\Users\\511232\\AppData\\Local\\Temp\\ipykernel_1820\\795136313.py:85: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(group_cols).apply(check_group)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'المؤشر'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_1820\\3530482148.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     90\u001b[39m     print(f\"Results saved to {output_filename}\")\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m'__main__'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     main()\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_1820\\3530482148.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     70\u001b[39m     final_df = pd.merge(df_general, df_nationality, on=[\u001b[33m'المؤشر'\u001b[39m, \u001b[33m'الدولة'\u001b[39m], how=\u001b[33m'left'\u001b[39m)\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'المنطقة'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m main_df.columns:\n\u001b[32m     73\u001b[39m         df_area = area_availability.reset_index(name=\u001b[33m'area_availability'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m         final_df = pd.merge(final_df, df_area, on=[\u001b[33m'المؤشر'\u001b[39m, \u001b[33m'الدولة'\u001b[39m], how=\u001b[33m'left'\u001b[39m)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         final_df[\u001b[33m'area_availability'\u001b[39m] = np.nan \u001b[38;5;66;03m# Add NaN column if area doesn't exist\u001b[39;00m\n\u001b[32m     77\u001b[39m \n",
      "\u001b[32mc:\\Users\\511232\\Desktop\\DSS\\DATA AVAILABILITY INTERACTIVE DASHBOARD\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\511232\\Desktop\\DSS\\DATA AVAILABILITY INTERACTIVE DASHBOARD\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\511232\\Desktop\\DSS\\DATA AVAILABILITY INTERACTIVE DASHBOARD\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1296\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1297\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1299\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1300\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1301\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\511232\\Desktop\\DSS\\DATA AVAILABILITY INTERACTIVE DASHBOARD\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'المؤشر'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Main function to run the entire analysis pipeline.\n",
    "    \"\"\"\n",
    "    # 1. Read in the main excel file and the criteria file\n",
    "    try:\n",
    "        # Using the path variables defined at the top of the script\n",
    "        main_df = pd.read_excel(MAIN_DATA_FILE)\n",
    "        criteria_df = pd.read_excel(CRITERIA_FILE)\n",
    "        print(\"Files read successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading files: {e}. Make sure they are in the correct directory.\")\n",
    "        return\n",
    "\n",
    "    # 2. Create the criteria dictionary (using Arabic names to match the main file)\n",
    "    criteria_dict_ar = create_criteria_dict(criteria_df, key_language='arabic')\n",
    "    print(f\"Criteria dictionary created with {len(criteria_dict_ar)} entries.\")\n",
    "    \n",
    "    # 3. Calculate the three availability columns, ensuring 'العدد' (Value) is not null.\n",
    "    \n",
    "    # General availability - now considers only rows with a valid value\n",
    "    print(\"Calculating general availability...\")\n",
    "    general_df = main_df[main_df['العدد'].notna()].copy()\n",
    "    general_availability = calculate_availability(\n",
    "        general_df,\n",
    "        group_cols=['المؤشر', 'الدولة'],\n",
    "        criteria_dict=criteria_dict_ar\n",
    "    )\n",
    "    \n",
    "    # Nationality availability - now also checks for a valid value\n",
    "    print(\"Calculating nationality availability...\")\n",
    "    # Filter for valid nationality data and a non-null value before calculating\n",
    "    nationality_df = main_df[\n",
    "        main_df['العدد'].notna() &\n",
    "        main_df['المواطنة'].notna() & \n",
    "        ~main_df['المواطنة'].isin(['Not applicable', 'غير مطابق', 'Total'])\n",
    "    ].copy()\n",
    "    nationality_availability = calculate_availability(\n",
    "        nationality_df,\n",
    "        group_cols=['المؤشر', 'الدولة'],\n",
    "        criteria_dict=criteria_dict_ar\n",
    "    )\n",
    "\n",
    "    # Area availability - now also checks for a valid value\n",
    "    print(\"Calculating area availability...\")\n",
    "    # Assuming the area column is named 'المنطقة'. If it exists:\n",
    "    if 'المنطقة' in main_df.columns:\n",
    "        area_df_filtered = main_df[\n",
    "            main_df['العدد'].notna() &\n",
    "            main_df['المنطقة'].notna() & \n",
    "            ~main_df['المنطقة'].isin(['Not applicable', 'غير مطابق', 'Total'])\n",
    "        ].copy()\n",
    "        area_availability = calculate_availability(\n",
    "            area_df_filtered,\n",
    "            group_cols=['المؤشر', 'الدولة'],\n",
    "            criteria_dict=criteria_dict_ar\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: 'المنطقة' (Area) column not found. Area availability will be empty.\")\n",
    "        # Create an empty series to avoid errors later\n",
    "        area_availability = pd.Series(dtype=int)\n",
    "    \n",
    "\n",
    "    # 4. Create the resulting table\n",
    "    # Convert series to dataframes for merging\n",
    "    df_general = general_availability.reset_index(name='general_availability')\n",
    "    df_nationality = nationality_availability.reset_index(name='nationality_availability')\n",
    "    \n",
    "    # Merge the general and nationality results first\n",
    "    final_df = pd.merge(df_general, df_nationality, on=['المؤشر', 'الدولة'], how='left')\n",
    "    \n",
    "    # This block now safely handles the merge for area availability.\n",
    "    # It checks if the 'area_availability' Series is empty before trying to merge.\n",
    "    if not area_availability.empty:\n",
    "        df_area = area_availability.reset_index(name='area_availability')\n",
    "        final_df = pd.merge(final_df, df_area, on=['المؤشر', 'الدولة'], how='left')\n",
    "    else:\n",
    "        # If there's no area data, just create the column with a placeholder.\n",
    "        final_df['area_availability'] = np.nan \n",
    "\n",
    "    # Fill any NaN values that resulted from the merge with 0 (since they are not available)\n",
    "    final_df.fillna(0, inplace=True)\n",
    "    # Convert availability columns to integers\n",
    "    for col in ['general_availability', 'nationality_availability', 'area_availability']:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].astype(int)\n",
    "\n",
    "    print(\"Final DataFrame created:\")\n",
    "    try:\n",
    "        print(final_df.head())\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"\\nNOTE: Could not display DataFrame head in the console due to character encoding issues.\")\n",
    "        print(\"This is a common issue with non-English characters on Windows terminals.\")\n",
    "        print(\"The data has been processed correctly and will be saved to the Excel file.\")\n",
    "\n",
    "\n",
    "    # 5. Save the result as excel\n",
    "    output_filename = 'availability_results.xlsx'\n",
    "    final_df.to_excel(output_filename, index=False, engine='openpyxl')\n",
    "    print(f\"\\nResults saved to {output_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5579f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78a4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9268ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21200a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd2282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a585f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffe68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
